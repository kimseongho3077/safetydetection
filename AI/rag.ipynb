{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# conda install langchain -c conda-forge\n",
    "# pip install langchain[all]\n",
    "# pip install llama-index\n",
    "# pip install openai\n",
    "# pip install pypdf\n",
    "# pip install chromadb\n",
    "# pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OpenAI API í‚¤ ì„¤ì •\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")  # í™˜ê²½ ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "print(api_key)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "import openai\n",
    "OPENAI_API_KEY =  os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_key= OPENAI_API_KEY\n",
    "# \"Private-Data\" í´ë” ë‚´ PDF ë¬¸ì„œ ë¡œë“œ\n",
    "resume = SimpleDirectoryReader(\"./ragdata\").load_data()\n",
    "\n",
    "# íŠ¸ë¦¬ ì¸ë±ìŠ¤(TreeIndex) ìƒì„±\n",
    "new_index =VectorStoreIndex.from_documents(resume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ê²€ìƒ‰ ê²°ê³¼:\n",
      "Maintain the patient's body temperature by covering them with a blanket and providing warm fluids in case of skin temperature issues during emergencies. Avoid giving anything to eat if the patient is unconscious.\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ ì¶œì²˜:\n",
      "- ì¶œì²˜: ì¶œì²˜ ì—†ìŒ, ì ìˆ˜: 0.8047402708982816\n",
      "- ì¶œì²˜: ì¶œì²˜ ì—†ìŒ, ì ìˆ˜: 0.7994013890722065\n"
     ]
    }
   ],
   "source": [
    "# ì¿¼ë¦¬ ì—”ì§„ ìƒì„±\n",
    "query_engine = new_index.as_query_engine()\n",
    "\n",
    "# ê²€ìƒ‰ì— ì‚¬ìš©í•  ìƒíƒœ ë° ì›ì¸ ì…ë ¥\n",
    "status_INPUT = \"ìœ„ê¸‰\"\n",
    "cause_INPUT = \"í”¼ë¶€ì˜¨ë„\"\n",
    "\n",
    "# ê²€ìƒ‰ì–´ ìƒì„±\n",
    "search_query = f\"{status_INPUT} ìƒíƒœì—ì„œ {cause_INPUT} ë¬¸ì œ ë°œìƒ ì‹œ í•´ê²° ë°©ë²•\"\n",
    "\n",
    "# ğŸ” ì¿¼ë¦¬ ì‹¤í–‰\n",
    "response = query_engine.query(search_query)\n",
    "\n",
    "# ğŸ”¥ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "print(response)\n",
    "\n",
    "# ğŸ”¥ ì‘ë‹µì´ ë‚˜ì˜¨ ë¬¸ì„œì˜ ì¶œì²˜ í™•ì¸ (ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)\n",
    "print(\"\\në¬¸ì„œ ì¶œì²˜:\")\n",
    "if hasattr(response, 'source_nodes') and response.source_nodes:\n",
    "    for node in response.source_nodes:\n",
    "        print(f\"- ì¶œì²˜: {node.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}, ì ìˆ˜: {node.score}\")\n",
    "else:\n",
    "    print(\"ì¶œì²˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# ì €ì¥ëœ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ë¡œë“œ\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "new_index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = new_index.as_query_engine()\n",
    "\n",
    "# response = query_engine.query(\"ktì˜ ì¥ì ì€?\")\n",
    "# print(response)\n",
    "\n",
    "\n",
    "# # ğŸ”¥ ì‘ë‹µì´ ë‚˜ì˜¨ ë¬¸ì„œì˜ ì¶œì²˜ í™•ì¸\n",
    "# print(response.source_nodes)  # ë¬¸ì„œ ì¶œì²˜ë¥¼ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: 1. í˜„ì¬ ìƒíƒœ: í”¼ë¶€ ì˜¨ë„ê°€ ìƒìŠ¹í•˜ì—¬ ê±´ê°•ìƒ ìœ„í—˜í•œ ìƒíƒœì…ë‹ˆë‹¤.\n",
      "2. ì›ì¸: í”¼ë¶€ ì˜¨ë„ ìƒìŠ¹\n",
      "3. ëŒ€ì²˜ ë°©ë²•: ì¦‰ì‹œ ì‹œì›í•œ ì¥ì†Œë¡œ ì´ë™í•˜ì‹­ì‹œì˜¤. ì°¨ê°€ìš´ ë¬¼ì´ë‚˜ ì•„ì´ìŠ¤íŒ©ì„ ì‚¬ìš©í•˜ì—¬ í”¼ë¶€ ì˜¨ë„ë¥¼ ë‚®ì¶”ì‹­ì‹œì˜¤. ë” ì´ìƒì˜ ì—´ë…¸ì¶œì„ í”¼í•˜ì‹­ì‹œì˜¤.\n",
      "4. ê²½ê³ : ìƒíƒœê°€ í˜¸ì „ë˜ì§€ ì•Šê±°ë‚˜ ì•…í™”ë˜ë©´ ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ì—¬ ì‘ê¸‰ì¹˜ë£Œë¥¼ ë°›ìœ¼ì‹­ì‹œì˜¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI LLM ëª¨ë¸ ìƒì„± (ì˜¬ë°”ë¥¸ ë°©ì‹)\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "template = \"\"\" \n",
    "í˜„ì¬ ìƒíƒœ: {status}\n",
    "ì›ì¸: {cause}\n",
    "ëŒ€ì²˜ ë°©ë²•: {method}\n",
    "\n",
    "ë„ˆëŠ” ì‘ê¸‰ìƒí™© ëŒ€ì‘ ì „ë¬¸ê°€ë‹¤. \n",
    "ê³ ê°ì˜ í˜„ì¬ ìƒíƒœë¥¼ ë‹¨í˜¸í•˜ê²Œ ì„¤ëª…í•˜ê³ , ì¦‰ê°ì ì¸ ëŒ€ì²˜ ë°©ë²•ì„ ì•ˆë‚´í•˜ë¼. \n",
    "'ë§Œì•½' ê°™ì€ ë¶ˆí™•ì‹¤í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , í•„ìš”í•œ ë§ë§Œ ì „ë‹¬í•˜ë¼.\n",
    "ìœ„ê¸‰í•œ ê²½ìš° 119ì— ì‹ ê³ í•´ì•¼ í•œë‹¤ëŠ” ê²½ê°ì‹¬ì„ ì‹¬ì–´ì¤˜ë¼. \n",
    "ì¶œë ¥ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•˜ë¼:\n",
    "\n",
    "1. í˜„ì¬ ìƒíƒœ ì„¤ëª… (ë‹¨í˜¸í•œ ë¬¸ì¥)\n",
    "2. ì›ì¸ ë¶„ì„ (ê°„ê²°í•˜ê²Œ)\n",
    "3. ì¦‰ê°ì ì¸ ëŒ€ì²˜ ë°©ë²• (ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ)\n",
    "4. ê²½ê³  ë¬¸êµ¬ (119 ì‹ ê³  í•„ìš” ì—¬ë¶€ ê°•ì¡°)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"status\", \"method\",\"cause\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ ê°’ ì„¤ì •\n",
    "status_INPUT = \"ìœ„ê¸‰\"\n",
    "cause_INPUT = \"í”¼ë¶€ì˜¨ë„\"\n",
    "method_INPUT = response.response\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‹¤í–‰\n",
    "final_prompt = prompt.format(status=status_INPUT,cause = cause_INPUT, method=method_INPUT )\n",
    "response = llm.invoke(final_prompt)\n",
    "\n",
    "print(f\"LLM Output: {response.content}\")  # âœ… ìµœì‹  ë°©ì‹ìœ¼ë¡œ ê²°ê³¼ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1. í˜„ì¬ ìƒíƒœ: í”¼ë¶€ ì˜¨ë„ê°€ ìƒìŠ¹í•˜ì—¬ ê±´ê°•ìƒ ìœ„í—˜í•œ ìƒíƒœì…ë‹ˆë‹¤.\\n2. ì›ì¸: í”¼ë¶€ ì˜¨ë„ ìƒìŠ¹\\n3. ëŒ€ì²˜ ë°©ë²•: ì¦‰ì‹œ ì‹œì›í•œ ì¥ì†Œë¡œ ì´ë™í•˜ì‹­ì‹œì˜¤. ì°¨ê°€ìš´ ë¬¼ì´ë‚˜ ì•„ì´ìŠ¤íŒ©ì„ ì‚¬ìš©í•˜ì—¬ í”¼ë¶€ ì˜¨ë„ë¥¼ ë‚®ì¶”ì‹­ì‹œì˜¤. ë” ì´ìƒì˜ ì—´ë…¸ì¶œì„ í”¼í•˜ì‹­ì‹œì˜¤.\\n4. ê²½ê³ : ìƒíƒœê°€ í˜¸ì „ë˜ì§€ ì•Šê±°ë‚˜ ì•…í™”ë˜ë©´ ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ì—¬ ì‘ê¸‰ì¹˜ë£Œë¥¼ ë°›ìœ¼ì‹­ì‹œì˜¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 298, 'total_tokens': 477, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_10f611d984', 'finish_reason': 'stop', 'logprobs': None} id='run-4f3e0729-43e5-4208-857d-f4dfa37814ee-0' usage_metadata={'input_tokens': 298, 'output_tokens': 179, 'total_tokens': 477, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print(type(response))  # ê°ì²´ íƒ€ì… í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: mpg321: not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# text = \"ì•ˆë…•í•˜ì„¸ìš”\"\n",
    "tts = gTTS(text=response.content, lang='ko')\n",
    "tts.save(\"output.mp3\")\n",
    "os.system(\"mpg321 output.mp3\")  # ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì‹¤í–‰ (ìœˆë„ìš°ì—ì„œëŠ” ì§ì ‘ ì‹¤í–‰ í•„ìš”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain langchain-openai openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

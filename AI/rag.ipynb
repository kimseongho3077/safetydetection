{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# conda install langchain -c conda-forge\n",
    "# pip install langchain[all]\n",
    "# pip install llama-index\n",
    "# pip install openai\n",
    "# pip install pypdf\n",
    "# pip install chromadb\n",
    "# pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")  # 환경 변수를 가져옴\n",
    "print(api_key)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "import openai\n",
    "OPENAI_API_KEY =  os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_key= OPENAI_API_KEY\n",
    "# \"Private-Data\" 폴더 내 PDF 문서 로드\n",
    "resume = SimpleDirectoryReader(\"./ragdata\").load_data()\n",
    "\n",
    "# 트리 인덱스(TreeIndex) 생성\n",
    "new_index =VectorStoreIndex.from_documents(resume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 검색 결과:\n",
      "Maintain the patient's body temperature by covering them with a blanket and providing warm fluids in case of skin temperature issues during emergencies. Avoid giving anything to eat if the patient is unconscious.\n",
      "\n",
      "📄 문서 출처:\n",
      "- 출처: 출처 없음, 점수: 0.8047402708982816\n",
      "- 출처: 출처 없음, 점수: 0.7994013890722065\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 엔진 생성\n",
    "query_engine = new_index.as_query_engine()\n",
    "\n",
    "# 검색에 사용할 상태 및 원인 입력\n",
    "status_INPUT = \"위급\"\n",
    "cause_INPUT = \"피부온도\"\n",
    "\n",
    "# 검색어 생성\n",
    "search_query = f\"{status_INPUT} 상태에서 {cause_INPUT} 문제 발생 시 해결 방법\"\n",
    "\n",
    "# 🔎 쿼리 실행\n",
    "response = query_engine.query(search_query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"검색 결과:\")\n",
    "print(response)\n",
    "\n",
    "# 응답이 나온 문서의 출처 확인 (예외 처리 추가)\n",
    "print(\"\\n문서 출처:\")\n",
    "if hasattr(response, 'source_nodes') and response.source_nodes:\n",
    "    for node in response.source_nodes:\n",
    "        print(f\"- 출처: {node.metadata.get('source', '출처 없음')}, 점수: {node.score}\")\n",
    "else:\n",
    "    print(\"출처 정보를 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# 저장된 인덱스를 다시 로드\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "new_index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = new_index.as_query_engine()\n",
    "\n",
    "# response = query_engine.query(\"kt의 장점은?\")\n",
    "# print(response)\n",
    "\n",
    "\n",
    "# # 🔥 응답이 나온 문서의 출처 확인\n",
    "# print(response.source_nodes)  # 문서 출처를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: 1. 현재 상태: 피부 온도가 상승하여 건강상 위험한 상태입니다.\n",
      "2. 원인: 피부 온도 상승\n",
      "3. 대처 방법: 즉시 시원한 장소로 이동하십시오. 차가운 물이나 아이스팩을 사용하여 피부 온도를 낮추십시오. 더 이상의 열노출을 피하십시오.\n",
      "4. 경고: 상태가 호전되지 않거나 악화되면 즉시 119에 신고하여 응급치료를 받으십시오.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI LLM 모델 생성 (올바른 방식)\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "template = \"\"\" \n",
    "현재 상태: {status}\n",
    "원인: {cause}\n",
    "대처 방법: {method}\n",
    "\n",
    "너는 응급상황 대응 전문가다. \n",
    "고객의 현재 상태를 단호하게 설명하고, 즉각적인 대처 방법을 안내하라. \n",
    "'만약' 같은 불확실한 표현을 사용하지 말고, 필요한 말만 전달하라.\n",
    "위급한 경우 119에 신고해야 한다는 경각심을 심어줘라. \n",
    "출력은 다음 형식으로 하라:\n",
    "\n",
    "1. 현재 상태 설명 (단호한 문장)\n",
    "2. 원인 분석 (간결하게)\n",
    "3. 즉각적인 대처 방법 (실용적이고 구체적으로)\n",
    "4. 경고 문구 (119 신고 필요 여부 강조)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"status\", \"method\",\"cause\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 사용자 입력 값 설정\n",
    "status_INPUT = \"위급\"\n",
    "cause_INPUT = \"피부온도\"\n",
    "method_INPUT = response.response\n",
    "\n",
    "# 최종 프롬프트 생성 및 실행\n",
    "final_prompt = prompt.format(status=status_INPUT,cause = cause_INPUT, method=method_INPUT )\n",
    "response = llm.invoke(final_prompt)\n",
    "\n",
    "print(f\"LLM Output: {response.content}\")  # ✅ 최신 방식으로 결과 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1. 현재 상태: 피부 온도가 상승하여 건강상 위험한 상태입니다.\\n2. 원인: 피부 온도 상승\\n3. 대처 방법: 즉시 시원한 장소로 이동하십시오. 차가운 물이나 아이스팩을 사용하여 피부 온도를 낮추십시오. 더 이상의 열노출을 피하십시오.\\n4. 경고: 상태가 호전되지 않거나 악화되면 즉시 119에 신고하여 응급치료를 받으십시오.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 298, 'total_tokens': 477, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_10f611d984', 'finish_reason': 'stop', 'logprobs': None} id='run-4f3e0729-43e5-4208-857d-f4dfa37814ee-0' usage_metadata={'input_tokens': 298, 'output_tokens': 179, 'total_tokens': 477, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print(type(response))  # 객체 타입 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: mpg321: not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# text = \"안녕하세요\"\n",
    "tts = gTTS(text=response.content, lang='ko')\n",
    "tts.save(\"output.mp3\")\n",
    "os.system(\"mpg321 output.mp3\")  # 리눅스 환경에서 실행 (윈도우에서는 직접 실행 필요)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain langchain-openai openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
